{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a4d3f78",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "## Setup notes\n",
    "- If installing TTS package on a venv, install propper cuda enabled torch otherwise default torch will be installed, preventing cuda from being used.\n",
    "- Go to \"TTS\\tts\\layers\\tortoise\\arch_utils.py\" replace references of LogitWarper to LogitsProcessor\n",
    "- Go to \"TTS\\tts\\models\\xtts.py then to function get_compatible_checkpoint_state_dict. On line 714: checkpoint = load_fsspec(model_path, map_location=torch.device(\"cpu\"))[\"model\"]. Add the argument 'weights_only = False\": checkpoint = load_fsspec(model_path, map_location=torch.device(\"cpu\"), weights_only = False)[\"model\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f2b8b23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12017\\Desktop\\NJIT\\DS677_852_Project\\src\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''Imports'''\n",
    "from trainer import Trainer, TrainerArgs\n",
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.layers.xtts.trainer.gpt_trainer import GPTArgs, GPTTrainer, GPTTrainerConfig, XttsAudioConfig\n",
    "from TTS.utils.manage import ModelManager\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58c75a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "12.4\n",
      "True\n",
      "NVIDIA GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "'''Display device used'''\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "print(torch.version.cuda)           # PyTorch's CUDA version (linked at build time)\n",
    "print(torch.cuda.is_available())    # Should be True\n",
    "print(torch.cuda.get_device_name()) # Your GPU's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "099c65f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths set.\n"
     ]
    }
   ],
   "source": [
    "'''DOWNLOADS'''\n",
    "# Get XTTS files\n",
    "CHECKPOINT_PATH = './XTTS-files/'\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# DVAE files\n",
    "DVAE_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/dvae.pth\"\n",
    "MEL_NORM_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/mel_stats.pth\"\n",
    "\n",
    "# Set the path to the downloaded files\n",
    "DVAE_CHECKPOINT = os.path.join(CHECKPOINT_PATH, os.path.basename(DVAE_LINK))\n",
    "MEL_NORM_FILE = os.path.join(CHECKPOINT_PATH, os.path.basename(MEL_NORM_LINK))\n",
    "\n",
    "# DVAE download if not exists\n",
    "if not os.path.isfile(DVAE_CHECKPOINT) or not os.path.isfile(MEL_NORM_FILE):\n",
    "    print(\" > Downloading DVAE files!\")\n",
    "    ModelManager._download_model_files([MEL_NORM_LINK, DVAE_LINK], CHECKPOINT_PATH, progress_bar=True)\n",
    "\n",
    "# XTTS v2.0 checkpoint\n",
    "TOKENIZER_FILE_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/vocab.json\"\n",
    "XTTS_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/model.pth\"\n",
    "\n",
    "# Transfer learning parameters. NOTE: Sets base model to use\n",
    "TOKENIZER_FILE = os.path.join(CHECKPOINT_PATH, os.path.basename(TOKENIZER_FILE_LINK))  # vocab.json\n",
    "XTTS_CHECKPOINT = os.path.join(CHECKPOINT_PATH, os.path.basename(XTTS_CHECKPOINT_LINK))  # model.pth\n",
    "\n",
    "# XTTS v2.0 download if not exists\n",
    "if not os.path.isfile(TOKENIZER_FILE) or not os.path.isfile(XTTS_CHECKPOINT):\n",
    "    print(\" > Downloading XTTS v2.0 files!\")\n",
    "    ModelManager._download_model_files(\n",
    "        [TOKENIZER_FILE_LINK, XTTS_CHECKPOINT_LINK], CHECKPOINT_PATH, progress_bar=True\n",
    "    )\n",
    "print(\"Paths set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08517a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Found 140 files in C:\\Users\\12017\\Desktop\\NJIT\\DS677_852_Project\\src\\datasets\\example_1\n"
     ]
    }
   ],
   "source": [
    "'''DATA LOADING'''\n",
    "# Set lang\n",
    "LANGUAGE ='en'\n",
    "# Set to folder name that contains metadata.csv and wavs dir (with the .wav examples)\n",
    "DATASET= \"example_1\"\n",
    "training_dir = f'./datasets/{DATASET}/' # change to folder w/ training examples\n",
    "\n",
    "# Dataset uses ljspeech format\n",
    "dataset_config = BaseDatasetConfig(\n",
    "    formatter=\"ljspeech\",\n",
    "    meta_file_train=\"metadata.csv\", # metadata file w/ transcriptions\n",
    "    language=LANGUAGE,\n",
    "    path=training_dir\n",
    ")\n",
    "\n",
    "# Turn off eval split. Will evaluate manually\n",
    "train_samples, eval_samples = load_tts_samples(\n",
    "    dataset_config,\n",
    "    eval_split=True,\n",
    "    eval_split_size=0.02 # Might change\n",
    ")\n",
    "\n",
    "\n",
    "'''MODIFY'''\n",
    "# Audio config\n",
    "audio_config = XttsAudioConfig(sample_rate=16000, dvae_sample_rate=16000, output_sample_rate=24000) \n",
    "\n",
    "# Speaker Reference: Match theses to the test sentences\n",
    "SPEAKER_TEXT = [\n",
    "    \"for a hundred yards, then, curving, was lost to view. Doubtless there was an outpost farther along. The other bank of the stream was open ground.\",\n",
    "    \"A gentle acclivity, topped with a stockade of vertical tree trunks, loopholed for rifles, with a single embrasure through, which protruded the muzzle of a brass cannon,\",\n",
    "    \"The lady had now brought the water, which the soldier drank. He thanked her ceremoniously, bowed to her husband, and rode away. An hour later, after nightfall,\"\n",
    "\n",
    "\n",
    "]\n",
    "SPEAKER_REFERENCE = [\n",
    "    F\"datasets/{DATASET}/wavs/chunk_0009.wav\",\n",
    "    F\"datasets/{DATASET}/wavs/chunk_0010.wav\",\n",
    "    F\"datasets/{DATASET}/wavs/chunk_0054.wav\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1db444e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set Model arguments'''\n",
    "model_args = GPTArgs(\n",
    "    max_conditioning_length=143677, # Audio used for conditioning latents should be less than this \n",
    "    min_conditioning_length=66150, # and more than this\n",
    "    debug_loading_failures=True,\n",
    "    max_wav_length=255995, # Set >= longest audio in dataset  \n",
    "    max_text_length=200, \n",
    "    mel_norm_file=MEL_NORM_FILE,\n",
    "    dvae_checkpoint=DVAE_CHECKPOINT,\n",
    "    xtts_checkpoint=XTTS_CHECKPOINT,  \n",
    "    tokenizer_file=TOKENIZER_FILE,\n",
    "    gpt_num_audio_tokens=1026, \n",
    "    gpt_start_audio_token=1024,\n",
    "    gpt_stop_audio_token=1025,\n",
    "    gpt_use_masking_gt_prompt_approach=True,\n",
    "    gpt_use_perceiver_resampler=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c84e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set up configuration file'''\n",
    "'''TRAINING CONFIG'''\n",
    "OUT_PATH = './training_outputs/'\n",
    "\n",
    "RUN_NAME = 'owl_speech_test'\n",
    "# RUN_NAME = f\"xttsv2_finetune_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "PROJECT_NAME = 'Owl Finetune'\n",
    "# DASHBOARD_LOGGER = 'wandb'\n",
    "LOGGER_URI = None\n",
    "\n",
    "OPTIMIZER_WD_ONLY_ON_WEIGHTS = True  \n",
    "\n",
    "BATCH_SIZE = 3 # 4 is common\n",
    "\n",
    "config = GPTTrainerConfig(\n",
    "    run_eval=True,\n",
    "    epochs = 4, # assuming you want to end training manually w/ keyboard interrupt\n",
    "    output_path=OUT_PATH,\n",
    "    model_args=model_args,\n",
    "    run_name=RUN_NAME,\n",
    "    project_name=PROJECT_NAME,\n",
    "    run_description=\"\"\"\n",
    "        GPT XTTS training\n",
    "        \"\"\",\n",
    "    # dashboard_logger=DASHBOARD_LOGGER,\n",
    "    wandb_entity=None,\n",
    "    # logger_uri=LOGGER_URI,\n",
    "    audio=audio_config,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    batch_group_size=48,\n",
    "    eval_batch_size=BATCH_SIZE,\n",
    "    num_loader_workers=0, # On Windows, num_loader_workers > 0 can break multiprocessing in PyTorch\n",
    "    eval_split_max_size=256, \n",
    "    print_step=50, \n",
    "    plot_step=100, \n",
    "    log_model_step=1000, \n",
    "    save_step=1000, # Needs to be an int\n",
    "    save_n_checkpoints=3, # Rotate last 3 checkpoints\n",
    "    save_checkpoints=True,\n",
    "    print_eval=True,\n",
    "    optimizer=\"AdamW\",\n",
    "    optimizer_wd_only_on_weights=OPTIMIZER_WD_ONLY_ON_WEIGHTS,\n",
    "    optimizer_params={\"betas\": [0.9, 0.96], \"eps\": 1e-8, \"weight_decay\": 1e-2},\n",
    "    lr=5e-06,  \n",
    "    lr_scheduler=\"MultiStepLR\",\n",
    "    lr_scheduler_params={\"milestones\": [50000 * 18, 150000 * 18, 300000 * 18], \"gamma\": 0.5, \"last_epoch\": -1},\n",
    "    test_sentences=[ \n",
    "        {\n",
    "            \"text\": SPEAKER_TEXT[0],\n",
    "            \"speaker_wav\": SPEAKER_REFERENCE[0], \n",
    "            \"language\": LANGUAGE,\n",
    "        },\n",
    "        {\n",
    "            \"text\": SPEAKER_TEXT[1],\n",
    "            \"speaker_wav\": SPEAKER_REFERENCE[1],\n",
    "            \"language\": LANGUAGE,\n",
    "        },\n",
    "        {\n",
    "            \"text\": SPEAKER_TEXT[2],\n",
    "            \"speaker_wav\": SPEAKER_REFERENCE[2],\n",
    "            \"language\": LANGUAGE,\n",
    "        }\n",
    "        \n",
    "    ],\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13eba573",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " > Training Environment:\n",
      " | > Backend: Torch\n",
      " | > Mixed precision: False\n",
      " | > Precision: float32\n",
      " | > Current device: 0\n",
      " | > Num. of GPUs: 1\n",
      " | > Num. of CPUs: 16\n",
      " | > Num. of Torch Threads: 1\n",
      " | > Torch seed: 1\n",
      " | > Torch CUDNN: True\n",
      " | > Torch CUDNN deterministic: False\n",
      " | > Torch CUDNN benchmark: False\n",
      " | > Torch TF32 MatMul: False\n",
      " > Start Tensorboard: tensorboard --logdir=./training_outputs/owl_speech_test-April-12-2025_04+01PM-dd3b630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> DVAE weights restored from: ./XTTS-files/dvae.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " > Model has 518442047 parameters\n"
     ]
    }
   ],
   "source": [
    "'''Set up Trainer'''\n",
    "# Init model \n",
    "model = GPTTrainer.init_from_config(config)\n",
    "\n",
    "# Init Trainer\n",
    "GRAD_ACUMM_STEPS = 252\n",
    "START_WITH_EVAL = True  \n",
    "\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(\n",
    "        restore_path=None, # Change to model path if resuming\n",
    "        skip_train_epoch=False,\n",
    "        start_with_eval=START_WITH_EVAL,\n",
    "        grad_accum_steps=GRAD_ACUMM_STEPS,\n",
    "    ),\n",
    "    config,\n",
    "    output_path=OUT_PATH,\n",
    "    model=model,\n",
    "    train_samples=train_samples,\n",
    "    eval_samples=eval_samples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffe4fb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 0/4\u001b[0m\n",
      " --> ./training_outputs/owl_speech_test-April-12-2025_04+01PM-dd3b630\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Filtering invalid eval samples!!\n",
      " > Total eval samples after filtering: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss_text_ce: 0.024175841361284256  (0.024175841361284256)\n",
      "     | > loss_mel_ce: 4.230667591094971  (4.230667591094971)\n",
      "     | > loss: 4.254843235015869  (4.254843235015869)\n",
      "\n",
      "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Synthesizing test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time: 0.0937800407409668 \u001b[0m(+0)\n",
      "     | > avg_loss_text_ce: 0.024175841361284256 \u001b[0m(+0)\n",
      "     | > avg_loss_mel_ce: 4.230667591094971 \u001b[0m(+0)\n",
      "     | > avg_loss: 4.254843235015869 \u001b[0m(+0)\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 1/4\u001b[0m\n",
      " --> ./training_outputs/owl_speech_test-April-12-2025_04+01PM-dd3b630\n",
      "\n",
      "\u001b[1m > TRAINING (2025-04-12 16:01:49) \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Sampling by language: dict_keys(['en'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m   --> TIME: 2025-04-12 16:01:54 -- STEP: 0/46 -- GLOBAL_STEP: 0\u001b[0m\n",
      "     | > loss_text_ce: 0.023332759737968445  (0.023332759737968445)\n",
      "     | > loss_mel_ce: 4.199986934661865  (4.199986934661865)\n",
      "     | > loss: 0.016759205609560013  (0.016759205609560013)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 5.4952  (5.495172739028931)\n",
      "     | > loader_time: 0.1356  (0.13561296463012695)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss_text_ce: 0.024010371416807175  (0.024010371416807175)\n",
      "     | > loss_mel_ce: 4.11047887802124  (4.11047887802124)\n",
      "     | > loss: 4.134489059448242  (4.134489059448242)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Synthesizing test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.0628664493560791 \u001b[0m(-0.030913591384887695)\n",
      "     | > avg_loss_text_ce:\u001b[92m 0.024010371416807175 \u001b[0m(-0.0001654699444770813)\n",
      "     | > avg_loss_mel_ce:\u001b[92m 4.11047887802124 \u001b[0m(-0.12018871307373047)\n",
      "     | > avg_loss:\u001b[92m 4.134489059448242 \u001b[0m(-0.12035417556762695)\n",
      "\n",
      " > BEST MODEL : ./training_outputs/owl_speech_test-April-12-2025_04+01PM-dd3b630\\best_model_46.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 2/4\u001b[0m\n",
      " --> ./training_outputs/owl_speech_test-April-12-2025_04+01PM-dd3b630\n",
      "\n",
      "\u001b[1m > TRAINING (2025-04-12 16:10:18) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-04-12 16:11:13 -- STEP: 4/46 -- GLOBAL_STEP: 50\u001b[0m\n",
      "     | > loss_text_ce: 0.023366305977106094  (0.023232044652104378)\n",
      "     | > loss_mel_ce: 4.010820388793945  (4.0120755434036255)\n",
      "     | > loss: 0.016008678823709488  (0.01601312612183392)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 2.5745  (2.7844334840774536)\n",
      "     | > loader_time: 0.0953  (0.0937570333480835)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error loading ./datasets/example_1/wavs\\﻿chunk_0000.wav (<class 'soundfile.LibsndfileError'>, LibsndfileError(2, \"Error opening './datasets/example_1/wavs\\\\\\\\\\\\ufeffchunk_0000.wav': \"), <traceback object at 0x00000261B5CD8B80>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss_text_ce: 0.023904411122202873  (0.023904411122202873)\n",
      "     | > loss_mel_ce: 4.023954391479492  (4.023954391479492)\n",
      "     | > loss: 4.047858715057373  (4.047858715057373)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Synthesizing test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.06782031059265137 \u001b[0m(+0.004953861236572266)\n",
      "     | > avg_loss_text_ce:\u001b[92m 0.023904411122202873 \u001b[0m(-0.00010596029460430145)\n",
      "     | > avg_loss_mel_ce:\u001b[92m 4.023954391479492 \u001b[0m(-0.08652448654174805)\n",
      "     | > avg_loss:\u001b[92m 4.047858715057373 \u001b[0m(-0.08663034439086914)\n",
      "\n",
      " > BEST MODEL : ./training_outputs/owl_speech_test-April-12-2025_04+01PM-dd3b630\\best_model_92.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 3/4\u001b[0m\n",
      " --> ./training_outputs/owl_speech_test-April-12-2025_04+01PM-dd3b630\n",
      "\n",
      "\u001b[1m > TRAINING (2025-04-12 16:21:05) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-04-12 16:23:19 -- STEP: 8/46 -- GLOBAL_STEP: 100\u001b[0m\n",
      "     | > loss_text_ce: 0.02607664465904236  (0.023935667937621474)\n",
      "     | > loss_mel_ce: 4.002424716949463  (4.033668577671051)\n",
      "     | > loss: 0.015986118465662003  (0.01610160490963608)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 2.5733  (4.16097429394722)\n",
      "     | > loader_time: 0.0908  (0.09121331572532654)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss_text_ce: 0.023824071511626244  (0.023824071511626244)\n",
      "     | > loss_mel_ce: 3.9446892738342285  (3.9446892738342285)\n",
      "     | > loss: 3.968513250350952  (3.968513250350952)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Synthesizing test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.07081103324890137 \u001b[0m(+0.00299072265625)\n",
      "     | > avg_loss_text_ce:\u001b[92m 0.023824071511626244 \u001b[0m(-8.033961057662964e-05)\n",
      "     | > avg_loss_mel_ce:\u001b[92m 3.9446892738342285 \u001b[0m(-0.07926511764526367)\n",
      "     | > avg_loss:\u001b[92m 3.968513250350952 \u001b[0m(-0.0793454647064209)\n",
      "\n",
      " > BEST MODEL : ./training_outputs/owl_speech_test-April-12-2025_04+01PM-dd3b630\\best_model_138.pth\n"
     ]
    }
   ],
   "source": [
    "'''TRAINING: manual interupts will set model to output saves at given checkpoints'''\n",
    "trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xtts_project_venv",
   "language": "python",
   "name": "xtts_project_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
