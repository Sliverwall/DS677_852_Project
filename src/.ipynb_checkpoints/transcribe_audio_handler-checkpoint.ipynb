{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adb0a0c0",
   "metadata": {},
   "source": [
    "# Transcribe_audio notebook\n",
    "\n",
    "## Purpose\n",
    "---\n",
    "- Uses openai's whisper-large-v3 model to take sample audio files, then automate the transcription process.\n",
    "- Coqui-Ai XTTS fine-tuning process requires a text-transcription for each audio file. If an audio sample does not have this, it would be difficult to write, by hand, the text needed.\n",
    "- Will be used when annotating speech from personal audio samples as well.\n",
    "---\n",
    "\n",
    "## How to use\n",
    "---\n",
    "- Requires torch and HuggingFace's transformers API to use the whisper-large-v3 model.\n",
    "- Define an import dir path where all your .wav audio files exist. \n",
    "- Define an output path for a csv file. Here, as each audio file is transcribed, its file-name and transcription will be written to the output CSV. This can be used as the metadata file for the fine-tuning process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e32938",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Requires FFMEG to be installed for whipser model'''\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a747a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load in whipster model using transformers api'''\n",
    "# Set device and torch data type\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "print(device)\n",
    "\n",
    "# Model identifier\n",
    "model_id = \"openai/whisper-large-v3\" # Was about 3G\n",
    "\n",
    "# Load the model and move it to the selected device\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch_dtype, \n",
    "    low_cpu_mem_usage=False, \n",
    "    use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Load the processor\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Create the speech recognition pipeline\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c9bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Step up paths for imput and output files'''\n",
    "# Define a path to an output CSV to save transcriptions\n",
    "outputPath = \"output/example_dataset.csv\"\n",
    "\n",
    "# Define where sample audio files are coming from\n",
    "audioDir = \"chunks/\"\n",
    "\n",
    "# Read in all files from chosen dir\n",
    "fileList = os.listdir(audioDir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2055cda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Transcribe sample files'''\n",
    "# Init list to hold all samples \n",
    "samples = []\n",
    "\n",
    "'''Loop here to go through multiple .wav files if needed'''\n",
    "for i in range(len(fileList)): \n",
    "    # Specify the path to your local .wav file\n",
    "    fileName = fileList[i]\n",
    "    audioPath = audioDir + fileName\n",
    "    # Msg to show transcription is proceeding\n",
    "    print(f\"Transcribing {fileName}...\")\n",
    "    # Run the pipeline on the .wav file\n",
    "    result = pipe(audioPath)\n",
    "    # LJ speech format (filename, transcript, normalised transcript)\n",
    "    samples.append((fileName.split('.')[0], result, result)) # no need to normalzied when fine-tuning. Just duplicate 2nd col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb2a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the samples list to output csv\n",
    "with open(outputPath, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "    # create csv writer\n",
    "    csvWriter = csv.writer(f, delimiter='|')\n",
    "    \n",
    "    # Note: No need for headers in LJ sppech format...\n",
    "    \n",
    "    # Write each sample to the CSV file\n",
    "    for entry in samples:\n",
    "        csvWriter.writerow(entry)\n",
    "\n",
    "print(\"Transcriptions written to:\", outputPath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
