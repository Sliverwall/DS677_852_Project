{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a4d3f78",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "## Setup notes\n",
    "- If installing TTS package on a venv, install propper cuda enabled torch otherwise default torch will be installed, preventing cuda from being used.\n",
    "- Go to \"TTS\\tts\\layers\\tortoise\\arch_utils.py\" replace references of LogitWarper to LogitsProcessor\n",
    "- Go to \"TTS\\tts\\models\\xtts.py then to function get_compatible_checkpoint_state_dict. On line 714: checkpoint = load_fsspec(model_path, map_location=torch.device(\"cpu\"))[\"model\"]. Add the argument 'weights_only = False\": checkpoint = load_fsspec(model_path, map_location=torch.device(\"cpu\"), weights_only = False)[\"model\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f2b8b23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12017\\Desktop\\NJIT\\DS677_852_Project\\src\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''Imports'''\n",
    "from trainer import Trainer, TrainerArgs\n",
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.layers.xtts.trainer.gpt_trainer import GPTArgs, GPTTrainer, GPTTrainerConfig, XttsAudioConfig\n",
    "from TTS.utils.manage import ModelManager\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58c75a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "12.4\n",
      "True\n",
      "NVIDIA GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "'''Display device used'''\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "print(torch.version.cuda)           # PyTorch's CUDA version (linked at build time)\n",
    "print(torch.cuda.is_available())    # Should be True\n",
    "print(torch.cuda.get_device_name()) # Your GPU's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "099c65f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths set.\n"
     ]
    }
   ],
   "source": [
    "'''DOWNLOADS'''\n",
    "# Get XTTS files\n",
    "CHECKPOINT_PATH = './XTTS-files/'\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# DVAE files\n",
    "DVAE_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/dvae.pth\"\n",
    "MEL_NORM_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/mel_stats.pth\"\n",
    "\n",
    "# Set the path to the downloaded files\n",
    "DVAE_CHECKPOINT = os.path.join(CHECKPOINT_PATH, os.path.basename(DVAE_LINK))\n",
    "MEL_NORM_FILE = os.path.join(CHECKPOINT_PATH, os.path.basename(MEL_NORM_LINK))\n",
    "\n",
    "# DVAE download if not exists\n",
    "if not os.path.isfile(DVAE_CHECKPOINT) or not os.path.isfile(MEL_NORM_FILE):\n",
    "    print(\" > Downloading DVAE files!\")\n",
    "    ModelManager._download_model_files([MEL_NORM_LINK, DVAE_LINK], CHECKPOINT_PATH, progress_bar=True)\n",
    "\n",
    "# XTTS v2.0 checkpoint\n",
    "TOKENIZER_FILE_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/vocab.json\"\n",
    "XTTS_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/model.pth\"\n",
    "\n",
    "# Transfer learning parameters. NOTE: Sets base model to use\n",
    "TOKENIZER_FILE = os.path.join(CHECKPOINT_PATH, os.path.basename(TOKENIZER_FILE_LINK))  # vocab.json\n",
    "XTTS_CHECKPOINT = os.path.join(CHECKPOINT_PATH, os.path.basename(XTTS_CHECKPOINT_LINK))  # model.pth\n",
    "\n",
    "# XTTS v2.0 download if not exists\n",
    "if not os.path.isfile(TOKENIZER_FILE) or not os.path.isfile(XTTS_CHECKPOINT):\n",
    "    print(\" > Downloading XTTS v2.0 files!\")\n",
    "    ModelManager._download_model_files(\n",
    "        [TOKENIZER_FILE_LINK, XTTS_CHECKPOINT_LINK], CHECKPOINT_PATH, progress_bar=True\n",
    "    )\n",
    "print(\"Paths set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08517a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Found 140 files in C:\\Users\\12017\\Desktop\\NJIT\\DS677_852_Project\\src\\datasets\\example_1\n"
     ]
    }
   ],
   "source": [
    "'''DATA LOADING'''\n",
    "# Set lang\n",
    "LANGUAGE ='en'\n",
    "# Set to folder name that contains metadata.csv and wavs dir (with the .wav examples)\n",
    "DATASET= \"example_1\"\n",
    "training_dir = f'./datasets/{DATASET}/' # change to folder w/ training examples\n",
    "\n",
    "# Dataset uses ljspeech format\n",
    "dataset_config = BaseDatasetConfig(\n",
    "    formatter=\"ljspeech\",\n",
    "    meta_file_train=\"metadata.csv\", # metadata file w/ transcriptions\n",
    "    language=LANGUAGE,\n",
    "    path=training_dir\n",
    ")\n",
    "\n",
    "# Turn off eval split. Will evaluate manually\n",
    "train_samples, eval_samples = load_tts_samples(\n",
    "    dataset_config,\n",
    "    eval_split=True,\n",
    "    eval_split_size=0.02 # Might change\n",
    ")\n",
    "\n",
    "\n",
    "'''MODIFY'''\n",
    "# Audio config\n",
    "audio_config = XttsAudioConfig(sample_rate=16000, dvae_sample_rate=16000, output_sample_rate=24000) \n",
    "\n",
    "# Speaker Reference: Match theses to the test sentences\n",
    "SPEAKER_TEXT = [\n",
    "    \"for a hundred yards, then, curving, was lost to view. Doubtless there was an outpost farther along. The other bank of the stream was open ground.\",\n",
    "    \"A gentle acclivity, topped with a stockade of vertical tree trunks, loopholed for rifles, with a single embrasure through, which protruded the muzzle of a brass cannon,\",\n",
    "    \"The lady had now brought the water, which the soldier drank. He thanked her ceremoniously, bowed to her husband, and rode away. An hour later, after nightfall,\"\n",
    "\n",
    "\n",
    "]\n",
    "SPEAKER_REFERENCE = [\n",
    "    F\"datasets/{DATASET}/wavs/chunk_0009.wav\",\n",
    "    F\"datasets/{DATASET}/wavs/chunk_0010.wav\",\n",
    "    F\"datasets/{DATASET}/wavs/chunk_0054.wav\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1db444e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set Model arguments'''\n",
    "model_args = GPTArgs(\n",
    "    max_conditioning_length=143677, # Audio used for conditioning latents should be less than this \n",
    "    min_conditioning_length=66150, # and more than this\n",
    "    debug_loading_failures=True,\n",
    "    max_wav_length=255995, # Set >= longest audio in dataset  \n",
    "    max_text_length=200, \n",
    "    mel_norm_file=MEL_NORM_FILE,\n",
    "    dvae_checkpoint=DVAE_CHECKPOINT,\n",
    "    xtts_checkpoint=XTTS_CHECKPOINT,  \n",
    "    tokenizer_file=TOKENIZER_FILE,\n",
    "    gpt_num_audio_tokens=1026, \n",
    "    gpt_start_audio_token=1024,\n",
    "    gpt_stop_audio_token=1025,\n",
    "    gpt_use_masking_gt_prompt_approach=True,\n",
    "    gpt_use_perceiver_resampler=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c84e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set up configuration file'''\n",
    "'''TRAINING CONFIG'''\n",
    "OUT_PATH = './training_outputs/'\n",
    "\n",
    "RUN_NAME = 'owl_speech_test'\n",
    "# RUN_NAME = f\"xttsv2_finetune_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "PROJECT_NAME = 'Owl Finetune'\n",
    "# DASHBOARD_LOGGER = 'wandb'\n",
    "LOGGER_URI = None\n",
    "\n",
    "OPTIMIZER_WD_ONLY_ON_WEIGHTS = True  \n",
    "\n",
    "BATCH_SIZE = 3 # 4 is common\n",
    "\n",
    "config = GPTTrainerConfig(\n",
    "    run_eval=True,\n",
    "    epochs = 4, # assuming you want to end training manually w/ keyboard interrupt\n",
    "    output_path=OUT_PATH,\n",
    "    model_args=model_args,\n",
    "    run_name=RUN_NAME,\n",
    "    project_name=PROJECT_NAME,\n",
    "    run_description=\"\"\"\n",
    "        GPT XTTS training\n",
    "        \"\"\",\n",
    "    # dashboard_logger=DASHBOARD_LOGGER,\n",
    "    wandb_entity=None,\n",
    "    # logger_uri=LOGGER_URI,\n",
    "    audio=audio_config,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    batch_group_size=48,\n",
    "    eval_batch_size=BATCH_SIZE,\n",
    "    num_loader_workers=0, # On Windows, num_loader_workers > 0 can break multiprocessing in PyTorch\n",
    "    eval_split_max_size=256, \n",
    "    print_step=50, \n",
    "    plot_step=100, \n",
    "    log_model_step=1000, \n",
    "    save_step=1000, # Needs to be an int\n",
    "    save_n_checkpoints=3, # Rotate last 3 checkpoints\n",
    "    save_checkpoints=True,\n",
    "    print_eval=True,\n",
    "    optimizer=\"AdamW\",\n",
    "    optimizer_wd_only_on_weights=OPTIMIZER_WD_ONLY_ON_WEIGHTS,\n",
    "    optimizer_params={\"betas\": [0.9, 0.96], \"eps\": 1e-8, \"weight_decay\": 1e-2},\n",
    "    lr=5e-06,  \n",
    "    lr_scheduler=\"MultiStepLR\",\n",
    "    lr_scheduler_params={\"milestones\": [50000 * 18, 150000 * 18, 300000 * 18], \"gamma\": 0.5, \"last_epoch\": -1},\n",
    "    test_sentences=[ \n",
    "        {\n",
    "            \"text\": SPEAKER_TEXT[0],\n",
    "            \"speaker_wav\": SPEAKER_REFERENCE[0], \n",
    "            \"language\": LANGUAGE,\n",
    "        },\n",
    "        {\n",
    "            \"text\": SPEAKER_TEXT[1],\n",
    "            \"speaker_wav\": SPEAKER_REFERENCE[1],\n",
    "            \"language\": LANGUAGE,\n",
    "        },\n",
    "        {\n",
    "            \"text\": SPEAKER_TEXT[2],\n",
    "            \"speaker_wav\": SPEAKER_REFERENCE[2],\n",
    "            \"language\": LANGUAGE,\n",
    "        }\n",
    "        \n",
    "    ],\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13eba573",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " > Training Environment:\n",
      " | > Backend: Torch\n",
      " | > Mixed precision: False\n",
      " | > Precision: float32\n",
      " | > Current device: 0\n",
      " | > Num. of GPUs: 1\n",
      " | > Num. of CPUs: 16\n",
      " | > Num. of Torch Threads: 1\n",
      " | > Torch seed: 1\n",
      " | > Torch CUDNN: True\n",
      " | > Torch CUDNN deterministic: False\n",
      " | > Torch CUDNN benchmark: False\n",
      " | > Torch TF32 MatMul: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> DVAE weights restored from: ./XTTS-files/dvae.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " > Start Tensorboard: tensorboard --logdir=./training_outputs/owl_speech_test-April-12-2025_03+54PM-dd3b630\n",
      "\n",
      " > Model has 518442047 parameters\n"
     ]
    }
   ],
   "source": [
    "'''Set up Trainer'''\n",
    "# Init model \n",
    "model = GPTTrainer.init_from_config(config)\n",
    "\n",
    "# Init Trainer\n",
    "GRAD_ACUMM_STEPS = 252\n",
    "START_WITH_EVAL = True  \n",
    "\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(\n",
    "        restore_path=None, # Change to model path if resuming\n",
    "        skip_train_epoch=False,\n",
    "        start_with_eval=START_WITH_EVAL,\n",
    "        grad_accum_steps=GRAD_ACUMM_STEPS,\n",
    "    ),\n",
    "    config,\n",
    "    output_path=OUT_PATH,\n",
    "    model=model,\n",
    "    train_samples=train_samples,\n",
    "    eval_samples=eval_samples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffe4fb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 0/4\u001b[0m\n",
      " --> ./training_outputs/owl_speech_test-April-12-2025_03+54PM-dd3b630\n",
      "\n",
      "\u001b[1m > TRAINING (2025-04-12 15:56:20) \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Sampling by language: dict_keys(['en'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m   --> TIME: 2025-04-12 15:56:22 -- STEP: 0/47 -- GLOBAL_STEP: 0\u001b[0m\n",
      "     | > loss_text_ce: 0.02458968386054039  (0.02458968386054039)\n",
      "     | > loss_mel_ce: 3.9973747730255127  (3.9973747730255127)\n",
      "     | > loss: 0.015960177406668663  (0.015960177406668663)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 0.7939  (0.7938776016235352)\n",
      "     | > loader_time: 0.7326  (0.7325732707977295)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error loading ./datasets/example_1/wavs\\﻿chunk_0000.wav (<class 'soundfile.LibsndfileError'>, LibsndfileError(2, \"Error opening './datasets/example_1/wavs\\\\\\\\\\\\ufeffchunk_0000.wav': \"), <traceback object at 0x00000260FC549900>)\n",
      "Ignoring sample ./datasets/example_1/wavs\\﻿chunk_0000.wav because it was already ignored before !!\n",
      " > Filtering invalid eval samples!!\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] The process cannot access the file because it is being used by another process: 'C:/Users/12017/Desktop/NJIT/DS677_852_Project/src/training_outputs/owl_speech_test-April-12-2025_03+54PM-dd3b630\\\\trainer_0_log.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\Desktop\\NJIT\\DS677_852_Project\\src\\venv\\lib\\site-packages\\trainer\\trainer.py:1833\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1832\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1833\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\Desktop\\NJIT\\DS677_852_Project\\src\\venv\\lib\\site-packages\\trainer\\trainer.py:1787\u001b[0m, in \u001b[0;36mTrainer._fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrun_eval:\n\u001b[1;32m-> 1787\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtest_delay_epochs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\Desktop\\NJIT\\DS677_852_Project\\src\\venv\\lib\\site-packages\\trainer\\trainer.py:1628\u001b[0m, in \u001b[0;36mTrainer.eval_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_loader \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1628\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_eval_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_assets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1630\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1631\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1633\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrun_eval\n\u001b[0;32m   1634\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1635\u001b[0m     )\n\u001b[0;32m   1637\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Desktop\\NJIT\\DS677_852_Project\\src\\venv\\lib\\site-packages\\trainer\\trainer.py:990\u001b[0m, in \u001b[0;36mTrainer.get_eval_dataloader\u001b[1;34m(self, training_assets, samples, verbose)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m loader\n\u001b[1;32m--> 990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_loader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    991\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    992\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_assets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\NJIT\\DS677_852_Project\\src\\venv\\lib\\site-packages\\trainer\\trainer.py:909\u001b[0m, in \u001b[0;36mTrainer._get_loader\u001b[1;34m(self, model, config, assets, is_eval, samples, verbose, num_gpus)\u001b[0m\n\u001b[0;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isimplemented(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_data_loader\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 909\u001b[0m         loader \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_loader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    910\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_gpus\u001b[49m\n\u001b[0;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28mlen\u001b[39m(loader) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    915\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ❗ len(DataLoader) returns 0. Make sure your dataset is not empty or len(dataset) > 0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\Desktop\\NJIT\\DS677_852_Project\\src\\venv\\lib\\site-packages\\TTS\\tts\\layers\\xtts\\trainer\\gpt_trainer.py:367\u001b[0m, in \u001b[0;36mGPTTrainer.get_data_loader\u001b[1;34m(self, config, assets, is_eval, samples, verbose, num_gpus, rank)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;66;03m# init dataloader\u001b[39;00m\n\u001b[1;32m--> 367\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mXTTSDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxtts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;66;03m# wait all the DDP process to be ready\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\NJIT\\DS677_852_Project\\src\\venv\\lib\\site-packages\\TTS\\tts\\layers\\xtts\\trainer\\dataset.py:77\u001b[0m, in \u001b[0;36mXTTSDataset.__init__\u001b[1;34m(self, config, samples, tokenizer, sample_rate, is_eval)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;66;03m# for evaluation load and check samples that are corrupted to ensures the reproducibility\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_eval_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\NJIT\\DS677_852_Project\\src\\venv\\lib\\site-packages\\TTS\\tts\\layers\\xtts\\trainer\\dataset.py:82\u001b[0m, in \u001b[0;36mXTTSDataset.check_eval_samples\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m new_samples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples:\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m'''TRAINING: manual interupts will set model to output saves at given checkpoints'''\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\NJIT\\DS677_852_Project\\src\\venv\\lib\\site-packages\\trainer\\trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1858\u001b[0m         os\u001b[38;5;241m.\u001b[39m_exit(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m     \u001b[43mremove_experiment_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1861\u001b[0m     traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[0;32m   1862\u001b[0m     sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\NJIT\\DS677_852_Project\\src\\venv\\lib\\site-packages\\trainer\\generic_utils.py:77\u001b[0m, in \u001b[0;36mremove_experiment_folder\u001b[1;34m(experiment_path)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m checkpoint_files:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mexists(experiment_path):\n\u001b[1;32m---> 77\u001b[0m         \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ! Run is removed from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, experiment_path)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Desktop\\NJIT\\DS677_852_Project\\src\\venv\\lib\\site-packages\\fsspec\\implementations\\local.py:189\u001b[0m, in \u001b[0;36mLocalFileSystem.rm\u001b[1;34m(self, path, recursive, maxdepth)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m osp\u001b[38;5;241m.\u001b[39mabspath(p) \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd():\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot delete current working directory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 189\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    191\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(p)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\shutil.py:750\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;66;03m# can't continue even if onerror hook returns\u001b[39;00m\n\u001b[0;32m    749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_rmtree_unsafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\shutil.py:620\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    618\u001b[0m             os\u001b[38;5;241m.\u001b[39munlink(fullname)\n\u001b[0;32m    619\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m--> 620\u001b[0m             \u001b[43monerror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    622\u001b[0m     os\u001b[38;5;241m.\u001b[39mrmdir(path)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\shutil.py:618\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 618\u001b[0m         \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    619\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    620\u001b[0m         onerror(os\u001b[38;5;241m.\u001b[39munlink, fullname, sys\u001b[38;5;241m.\u001b[39mexc_info())\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] The process cannot access the file because it is being used by another process: 'C:/Users/12017/Desktop/NJIT/DS677_852_Project/src/training_outputs/owl_speech_test-April-12-2025_03+54PM-dd3b630\\\\trainer_0_log.txt'"
     ]
    }
   ],
   "source": [
    "'''TRAINING: manual interupts will set model to output saves at given checkpoints'''\n",
    "trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xtts_project_venv",
   "language": "python",
   "name": "xtts_project_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
