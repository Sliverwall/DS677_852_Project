{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c188d968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "import webvtt\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import csv\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8baa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yt_download(url):\n",
    "    \"\"\"\n",
    "    Downloads a wav file from given YouTube url.\n",
    "    \"\"\"\n",
    "    # Create audio folder\n",
    "    audio_dir = f\"./audio/\"\n",
    "    if not os.path.exists(audio_dir):\n",
    "        os.makedirs(audio_dir)\n",
    "\n",
    "    # Get video title (used later to create dir)\n",
    "    result = subprocess.run(\n",
    "        [\"yt-dlp\", \"--get-title\", url],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    title = result.stdout.strip()\n",
    "\n",
    "    yt_audio_path = os.path.join(audio_dir, f\"{title}.wav\")\n",
    "\n",
    "    subprocess.run([\n",
    "        \"yt-dlp\",\n",
    "        \"-f\", \"bestaudio\",\n",
    "        \"--extract-audio\",\n",
    "        \"--audio-format\", \"wav\", # convert to wav as this is XTTS prefered format\n",
    "        \"-o\", yt_audio_path,\n",
    "        url\n",
    "    ])\n",
    "\n",
    "    return yt_audio_path, title\n",
    "\n",
    "def chunk_audio(audio_path, video_title):\n",
    "    # Create dataset folder\n",
    "    dataset_dir = f\"./datasets/{video_title}\"\n",
    "    if not os.path.exists(dataset_dir):\n",
    "        os.makedirs(dataset_dir)\n",
    "\n",
    "    wavs_dir = os.path.join(dataset_dir, 'wavs')\n",
    "    if not os.path.exists(wavs_dir):\n",
    "        os.makedirs(wavs_dir)\n",
    "    \n",
    "    # Load audio\n",
    "    audio = AudioSegment.from_wav(audio_path)  \n",
    "\n",
    "    # Get the total length of time, in ms, for the audio file\n",
    "    totalLength = len(audio)\n",
    "\n",
    "    # Define length of time for each chunk in seconds\n",
    "    chunk_length = 10 * 1000  # seconds in ms\n",
    "\n",
    "    # Get the total amount of chunks needed to divide the audio file\n",
    "    num_chunks = totalLength // chunk_length + int(totalLength % chunk_length != 0)\n",
    "\n",
    "    # display expected number of chunks\n",
    "    print(f'Expected number of chunks: {num_chunks}')\n",
    "\n",
    "    # Extract the needed number of audio chunks from the audio file\n",
    "    print(f'Chunks saving into {wavs_dir}')\n",
    "    for i in range(num_chunks):\n",
    "        # Get the start and end time for the chunk\n",
    "        start = i * chunk_length\n",
    "        end = min((i + 1) * chunk_length, totalLength)\n",
    "\n",
    "        # Extract audio and output labeled chunk into ouput dir as a wav file\n",
    "        chunk = audio[start:end]\n",
    "        chunk.export(os.path.join(wavs_dir, f\"chunk_{i:04}.wav\"), format=\"wav\")\n",
    "\n",
    "    return dataset_dir, wavs_dir\n",
    "\n",
    "def transcribe_wavs(dataset_dir, wavs_dir):\n",
    "    # Set device and torch data type\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    print(f'Using device: {device}')\n",
    "\n",
    "    # Define chunk size used in seconds\n",
    "    chunk_size = 10\n",
    "\n",
    "    # Model identifier\n",
    "    model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "    # Load the model and move it to the selected device\n",
    "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id, \n",
    "        torch_dtype=torch_dtype, \n",
    "        low_cpu_mem_usage=False, \n",
    "        use_safetensors=True\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    # Load the processor\n",
    "    processor = AutoProcessor.from_pretrained(model_id, language='en')\n",
    "\n",
    "    # Add a new special pad token (string) to the tokenizer\n",
    "    if processor.tokenizer.pad_token_id == processor.tokenizer.eos_token_id:\n",
    "        processor.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        processor.tokenizer.pad_token_id = processor.tokenizer.convert_tokens_to_ids('[PAD]')\n",
    "\n",
    "    # Create the speech recognition pipeline\n",
    "    pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        chunk_length_s=chunk_size,\n",
    "        batch_size=32, \n",
    "        return_timestamps=True,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Define a path to an output CSV to save transcriptions\n",
    "    csv_path = os.path.join(dataset_dir, \"metadata.csv\")\n",
    "    file_list = sorted(os.listdir(wavs_dir))\n",
    "\n",
    "    # Create full audio paths\n",
    "    audio_paths = [os.path.join(wavs_dir, file) for file in file_list]\n",
    "\n",
    "    # Run the pipeline on all files in batch\n",
    "    results = pipe(audio_paths)\n",
    "\n",
    "    # Format results in LJ Speech style\n",
    "    samples = [\n",
    "        (os.path.splitext(os.path.basename(audio_paths[i]))[0], \n",
    "         results[i][\"text\"], \n",
    "         results[i][\"text\"])\n",
    "        for i in range(len(results))\n",
    "    ]\n",
    "\n",
    "    # Write the samples list to output csv\n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "        csv_writer = csv.writer(f, delimiter='|')\n",
    "        for entry in samples:\n",
    "            csv_writer.writerow(entry)\n",
    "\n",
    "    return csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a290b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4102ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get URL\n",
    "url = https://www.youtube.com/watch?v=o8clTtGtNVI\n",
    "\n",
    "print('Downloading YouTube video...')\n",
    "yt_audio_path, title = yt_download(url)\n",
    "print(f'YouTube video: {title} downloaded.')\n",
    "\n",
    "print('Chunking audio...')\n",
    "dataset_dir, wavs_dir = chunk_audio(yt_audio_path, title)\n",
    "print('Audio chunking complete.')\n",
    "\n",
    "print('Transcribing wavs...')\n",
    "csv_path = transcribe_wavs(dataset_dir, wavs_dir)\n",
    "print(\"Transcriptions written to:\", csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xtts_project_venv",
   "language": "python",
   "name": "xtts_project_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
